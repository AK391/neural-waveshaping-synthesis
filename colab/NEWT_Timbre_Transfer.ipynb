{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NEWT Timbre Transfer",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anon9119/anon-project-1/blob/main/colab/NEWT_Timbre_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Yin7gZA80TBM"
      },
      "source": [
        "#@title Neural Waveshaping Synthesis: Timbre Transfer Demo\n",
        "#@markdown _by [Ben Hayes](https://benhayes.net/)_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSGz_-2IOMIr"
      },
      "source": [
        "#Â Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "zoJjEMHQuviu"
      },
      "source": [
        "#@title Install dependencies\n",
        "#@markdown \n",
        "!pip install torchtext==0.9.1 pytorch-lightning\n",
        "!pip install auraloss==0.2.1 black==20.8b1 click==7.1.2 gin-config==0.4.0 librosa==0.8.0 numpy==1.20.1 resampy==0.2.2 scipy==1.6.1 torchcrepe==0.0.12 wandb\n",
        "!pip install youtube-dl  # More recent version\n",
        "!pip install -q https://github.com/tugstugi/dl-colab-notebooks/archive/colab_utils.zip\n",
        "!rm -rf anon-project-1\n",
        "!git clone https://github.com/anon9119/anon-project-1.git\n",
        "!cd anon-project-1\n",
        "!python -m pip install git+https://github.com/anon9119/anon-project-1.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "XRR5iINku3LA"
      },
      "source": [
        "#@title Make imports\n",
        "\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from dl_colab_notebooks.audio import record_audio\n",
        "import gin\n",
        "from google.colab import files\n",
        "import IPython.display as ipd\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "import torch\n",
        "\n",
        "from neural_waveshaping_synthesis.data.utils.loudness_extraction import extract_perceptual_loudness\n",
        "from neural_waveshaping_synthesis.data.utils.mfcc_extraction import extract_mfcc\n",
        "from neural_waveshaping_synthesis.data.utils.f0_extraction import extract_f0_with_crepe\n",
        "from neural_waveshaping_synthesis.data.utils.preprocess_audio import preprocess_audio, convert_to_float32_audio, make_monophonic, resample_audio\n",
        "from neural_waveshaping_synthesis.models.neural_waveshaping import NeuralWaveshaping\n",
        "\n",
        "gin.constant(\"device\", \"cuda\")\n",
        "gin.parse_config_file(\"anon-project-1/gin/models/newt.gin\")\n",
        "gin.parse_config_file(\"anon-project-1/gin/data/urmp_4second_crepe.gin\")\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "checkpoints = dict(Violin=\"vn\", Flute=\"fl\", Trumpet=\"tpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "I_HqDjn-7V2v"
      },
      "source": [
        "#@title Load Checkpoint { run: \"auto\" }\n",
        "#@markdown Choose from one of three pretrained checkpoints. In future you will be able to upload your own checkpoints too.\n",
        "selected_checkpoint_name = \"Violin\" #@param [\"Violin\", \"Flute\", \"Trumpet\"]\n",
        "selected_checkpoint = checkpoints[selected_checkpoint_name]\n",
        "\n",
        "checkpoint_path = os.path.join(\n",
        "    \"anon-project-1/checkpoints/nws\", selected_checkpoint)\n",
        "model = NeuralWaveshaping.load_from_checkpoint(\n",
        "    os.path.join(checkpoint_path, \"last.ckpt\")).to(device)\n",
        "model.eval()\n",
        "data_mean = np.load(\n",
        "    os.path.join(checkpoint_path, \"data_mean.npy\"))\n",
        "data_std = np.load(\n",
        "    os.path.join(checkpoint_path, \"data_std.npy\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0fzadC0Npiu"
      },
      "source": [
        "# Audio Input\n",
        "\n",
        "You now have a few options for getting source audio into the model.\n",
        "Whichever you choose, monophonic audio will give you best results. Polyphony is likely to result in chaos.\n",
        "\n",
        "You only need to run one of these cells. Whichever one you ran last will be used as the model input. When you're done, jump down to **Prepare Audio** below.\n",
        "\n",
        "To start with, why not jump in with the pre-populated YouTube URL?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gSihCDrj7clT"
      },
      "source": [
        "#@title 1. Get Audio from YouTube\n",
        "\n",
        "#@markdown It's hard to beat the default video link...\n",
        "\n",
        "youtube_url = \"https://www.youtube.com/watch?v=dYvPCgcFDIo\" #@param\n",
        "start_in_seconds = 6.5 #@param {type: \"number\"}\n",
        "length_in_seconds = 20.0 #@param {type: \"number\"}\n",
        "\n",
        "!rm *.wav\n",
        "!youtube-dl --extract-audio --audio-format wav {youtube_url} #-o yt_audio.wav\n",
        "!mv *.wav yt_audio.wav\n",
        "\n",
        "rate, audio = wavfile.read(\"yt_audio.wav\")\n",
        "audio = convert_to_float32_audio(make_monophonic(audio))\n",
        "audio = audio[int(rate * start_in_seconds):int(rate * (start_in_seconds + length_in_seconds))]\n",
        "audio = resample_audio(audio, rate, model.sample_rate)\n",
        "ipd.Audio(audio, rate=model.sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaMg2gA7Rdwv"
      },
      "source": [
        "OR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "8w09hkUJOorz"
      },
      "source": [
        "#@title 2. Upload an audio file\n",
        "#@markdown For now, only .wav files are supported.\n",
        "\n",
        "!rm -rf *.wav\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "rate, audio = wavfile.read(file_name)\n",
        "audio = convert_to_float32_audio(make_monophonic(audio))\n",
        "audio = resample_audio(audio, rate, model.sample_rate)\n",
        "ipd.Audio(audio, rate=model.sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv07B0plRfkX"
      },
      "source": [
        "OR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "fL2_zxLaRgfn"
      },
      "source": [
        "#@title 3. Record audio\n",
        "#@markdown Try singing or whistling into the microphone and becoming an instrument yourself!\n",
        "\n",
        "record_seconds = 10 #@param {type: \"number\"}\n",
        "audio = record_audio(sample_rate=model.sample_rate)\n",
        "ipd.Audio(audio, rate=model.sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANL8C7JzOGMC"
      },
      "source": [
        "# Prepare Audio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "oxM8OhUR_mJ8"
      },
      "source": [
        "#@title Extract Audio Features\n",
        "#@markdown Here we extract F0 using CREPE and A-weighted loudness.\n",
        "\n",
        "f0, confidence = extract_f0_with_crepe(audio)\n",
        "loudness = extract_perceptual_loudness(audio)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "30CPJ2CQHJu2"
      },
      "source": [
        "#@title Adjust Control Signals { run: \"auto\" }\n",
        "#@markdown Our source audio might not quite match the characteristics of the training audio, so let's adjust it to fit\n",
        "octave_shift = -1 #@param {type: \"slider\", min: -4, max: 4, step: 1}\n",
        "loudness_floor = 0 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "loudness_scale = 0.5 #@param {type: \"slider\", min: 0, max: 2, step: 0.01}\n",
        "\n",
        "f0_shifted = f0 * (2 ** octave_shift)\n",
        "loudness_floored = loudness * (loudness > loudness_floor) - loudness_floor\n",
        "loudness_scaled = loudness_floored * loudness_scale\n",
        "# loudness = loudness * (confidence > 0.4)\n",
        "\n",
        "f0_norm = (f0_shifted - data_mean[0]) / data_std[0]\n",
        "loud_norm = (loudness_scaled - data_mean[1]) / data_std[1]\n",
        "\n",
        "f0_t = torch.tensor(f0_shifted, device=device).float()\n",
        "f0_norm_t = torch.tensor(f0_norm, device=device).float()\n",
        "loud_norm_t = torch.tensor(loud_norm, device=device).float()\n",
        "\n",
        "control = torch.stack((f0_norm_t, loud_norm_t), dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df-4FbaTOImG"
      },
      "source": [
        "# Generation Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hBq3VDPz_u9h"
      },
      "source": [
        "#@title Synthesise Audio!\n",
        "#@markdown Finally, run this cell to get some audio from the model.\n",
        "start_time = time.time()\n",
        "out = model(f0_t.expand(1, 1, -1), control.unsqueeze(0))\n",
        "run_time = time.time() - start_time\n",
        "rtf = (audio.shape[-1] / model.sample_rate) / run_time\n",
        "print(\"Audio generated in %.2f seconds. That's %.1f times faster than the real time threshold!\" % (run_time, rtf))\n",
        "ipd.Audio(out.detach().cpu().numpy(), rate=model.sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}